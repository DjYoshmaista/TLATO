repo_handlerORIG.py:
I. Import Statements
    I-1.) Standard Library Imports:
        I-1-a.) import csv:
            - Purpose/Usage: Provides functionality to work with CSV (Comma Separated Values) files.
            - Actions: Enables reading from and writing to CSV files.
            - Referenced/Called by: Not directly used in the provided snippet of `repo_handlerORIG.py` but available for use.
        I-1-b.) import hashlib:
            - Purpose/Usage: Provides various hashing algorithms.
            - Actions: Used for generating checksums/hashes of file content (e.g., MD5, SHA256).
            - Referenced/Called by: Implicitly used by `src.utils.hashing.*` which is imported. The `_calculate_hashes` method relies on this functionality via the imported hashing utilities.
        I-1-c.) import collections: (Appears twice)
            - Purpose/Usage: Provides specialized container datatypes.
            - Actions: Offers alternatives to Python's general-purpose built-in containers like `dict`, `list`, `set`, and `tuple`. `defaultdict` is used in `find_duplicate_files`.
            - Referenced/Called by: `collections.defaultdict` in `find_duplicate_files`.
        I-1-d.) import io: (Appears twice)
            - Purpose/Usage: Provides tools for working with various I/O streams.
            - Actions: Can be used for in-memory streams, raw I/O, buffered I/O, and text I/O.
            - Referenced/Called by: Not directly used in the provided `repo_handlerORIG.py` but available.
        I-1-e.) import inspect:
            - Purpose/Usage: Provides tools for introspection, allowing examination of live objects like modules, classes, functions, and tracebacks.
            - Actions: Can get information about function arguments, source code, class hierarchy, etc.
            - Referenced/Called by: Not directly used in the provided `repo_handlerORIG.py` but available.
        I-1-f.) import json:
            - Purpose/Usage: Provides functionality for working with JSON (JavaScript Object Notation) data.
            - Actions: Enables encoding Python objects as JSON strings (serialization) and decoding JSON strings into Python objects (deserialization).
            - Referenced/Called by: `_load_index` (json.load), `_save_index` (json.dump).
        I-1-g.) import os:
            - Purpose/Usage: Provides a way of using operating system dependent functionality.
            - Actions: Includes functions for file system navigation (e.g., `os.path.join`, `os.makedirs`, `os.remove`, `os.path.exists`, `os.path.getsize`, `os.path.abspath`, `os.path.relpath`, `os.path.dirname`), process management, etc.
            - Referenced/Called by: `_ensure_index_file`, `_get_relative_path`, `add_file`, `remove_file`, `compress_file`, `decompress_file`, `backup_repository_index`.
        I-1-h.) import random:
            - Purpose/Usage: Implements pseudo-random number generators for various distributions.
            - Actions: Can generate random numbers, make random choices, shuffle sequences.
            - Referenced/Called by: Not directly used in the provided `repo_handlerORIG.py` but available.
        I-1-i.) import shutil:
            - Purpose/Usage: Offers high-level file operations.
            - Actions: Includes functions for copying, moving, and removing files and directories.
            - Referenced/Called by: `backup_repository_index` (shutil.copy2).
        I-1-j.) import time:
            - Purpose/Usage: Provides various time-related functions.
            - Actions: Can be used for getting the current time, sleeping, measuring time intervals.
            - Referenced/Called by: `add_file` (time.time for performance metrics, though commented out).
        I-1-k.) from concurrent.futures import ThreadPoolExecutor, as_completed:
            - Purpose/Usage: Provides a high-level interface for asynchronously executing callables.
            - Actions: `ThreadPoolExecutor` creates a pool of threads for executing tasks concurrently. `as_completed` returns an iterator that yields futures as they complete.
            - Referenced/Called by: `add_files_threaded`.
        I-1-l.) from datetime import datetime as dt, timezone:
            - Purpose/Usage: Supplies classes for manipulating dates and times.
            - Actions: `dt` is used for getting current timestamps. `timezone` for timezone-aware datetime objects.
            - Referenced/Called by: `add_file` (dt.now(timezone.utc)), `backup_repository_index` (dt.now().strftime).
        I-1-m.) from pathlib import Path:
            - Purpose/Usage: Provides an object-oriented approach to file system paths.
            - Actions: Simplifies path manipulations, checking existence, reading/writing files, iterating directory contents.
            - Referenced/Called by: Extensively throughout the `DataRepository` class for path operations.
        I-1-n.) from typing import Any, Dict, List, Optional, Union, Set, Tuple:
            - Purpose/Usage: Provides support for type hints.
            - Actions: Improves code readability and allows static type checking. These types are used in function signatures and variable annotations.
            - Referenced/Called by: Throughout the `DataRepository` class in type hints.
        I-1-o.) import sys:
            - Purpose/Usage: Provides access to system-specific parameters and functions.
            - Actions: Used here to check if `psutil` is loaded.
            - Referenced/Called by: `PSUTIL_AVAILABLE` global constant definition.
        I-1-p.) import threading:
            - Purpose/Usage: Provides direct, low-level access to threading.
            - Actions: Used here for creating a lock (`self.index_lock`).
            - Referenced/Called by: `DataRepository.__init__`.

    I-2.) Third-Party Library Imports:
        I-2-a.) import numpy as np:
            - Purpose/Usage: Fundamental package for numerical computation in Python.
            - Actions: Provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.
            - Referenced/Called by: Not directly used in the provided `repo_handlerORIG.py` but available.
        I-2-b.) import pandas as pd:
            - Purpose/Usage: Provides high-performance, easy-to-use data structures and data analysis tools.
            - Actions: Offers DataFrame objects for data manipulation with integrated indexing.
            - Referenced/Called by: Not directly used in the provided `repo_handlerORIG.py` but available.
        I-2-c.) from tqdm import tqdm:
            - Purpose/Usage: Provides a fast, extensible progress bar for loops.
            - Actions: Wraps an iterable to print a smart progress meter.
            - Referenced/Called by: `add_files_threaded`, `verify_checksums`.
        I-2-d.) import psutil:
            - Purpose/Usage: A cross-platform library for retrieving information on running processes and system utilization (CPU, memory, disks, network, sensors).
            - Actions: Used here to get memory usage.
            - Referenced/Called by: `_log_memory_usage` (if `PSUTIL_AVAILABLE` is True).
        I-2-e.) import zstandard as zstd: (Appears twice, second time in a try-except block, which is redundant if the first one succeeds)
            - Purpose/Usage: Python bindings for Zstandard compression algorithm.
            - Actions: Used for file compression and decompression.
            - Referenced/Called by: `compress_file`, `decompress_file`.

    I-3.) Project-Specific Imports:
        I-3-a.) from src.utils.helpers import _get_max_workers, _generate_file_paths, _get_file_metadata:
            - Purpose/Usage: Imports helper functions from a utility module within the project.
                - `_get_max_workers`: Likely determines an optimal number of worker threads.
                - `_generate_file_paths`: Likely generates a list of file paths from a given directory/pattern.
                - `_get_file_metadata`: Likely extracts metadata from a file.
            - Actions: These functions encapsulate common utility operations.
            - Referenced/Called by:
                - `_get_max_workers`: `add_files_threaded`.
                - `_generate_file_paths`: Not directly used in `repo_handlerORIG.py` but available.
                - `_get_file_metadata`: Not directly used by name in `repo_handlerORIG.py` but similar functionality is in `add_file`.
        I-3-b.) from src.utils.logger import configure_logging, log_statement:
            - Purpose/Usage: Imports logging configuration and a custom logging function.
                - `configure_logging`: Sets up the logging system (e.g., format, level, handlers).
                - `log_statement`: A custom function for emitting log messages.
            - Actions: Centralizes logging setup and message emission.
            - Referenced/Called by: `configure_logging()` (called globally), `log_statement` (used throughout the class for logging).
        I-3-c.) from src.utils.hashing import *:
            - Purpose/Usage: Imports all functions/constants from a custom hashing utility module. Likely contains `hash_file`.
            - Actions: Provides file hashing capabilities.
            - Referenced/Called by: `_calculate_hashes` (uses `hash_file` from this import).
        I-3-d.) from src.utils.config import load_config: (within a try-except block)
            - Purpose/Usage: Imports a function to load project configuration.
            - Actions: Loads configuration settings, possibly from a file or environment variables.
            - Referenced/Called by: `load_config()` (called globally). It's also used by `_determine_processed_path` via `app_state`.
        I-3-e.) import cudf: (within a try-except block)
            - Purpose/Usage: GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.
            - Actions: If available, enables GPU-accelerated data processing.
            - Referenced/Called by: Global `GPU_AVAILABLE` flag is set based on its import success. Not directly used in methods of `repo_handlerORIG.py`.
        I-3-f.) import cupy: (within a try-except block)
            - Purpose/Usage: NumPy-compatible array library for GPU-accelerated computing.
            - Actions: If available, enables GPU-accelerated numerical computations.
            - Referenced/Called by: Global `GPU_AVAILABLE` flag is set based on its import success. Not directly used in methods of `repo_handlerORIG.py`.
        I-3-g.) from src.data.constants import *: (within a try-except block)
            - Purpose/Usage: Imports all constants from a project-specific constants module. Expected to contain `LOG_INS`.
            - Actions: Makes global constants available.
            - Referenced/Called by: Used for `LOG_INS` in `log_statement` calls. `CONSTANTS_AVAILABLE` flag is set.

II. Global Constants and Configurations
    II-1.) `PSUTIL_AVAILABLE = True if psutil in sys.modules else False`
        - Purpose/Usage: Checks if the `psutil` module has been successfully imported and is available in the current Python environment.
        - Action: Sets a boolean flag `PSUTIL_AVAILABLE`. This flag is used by `_log_memory_usage` to conditionally execute psutil-dependent code.
        - Mathematical/Logical Formula: `PSUTIL_AVAILABLE := (psutil \in sys.modules)`
        - Assessment: Correctly determines if `psutil` is loaded.

    II-2.) `configure_logging()`
        - Purpose/Usage: Initializes the logging system for the application.
        - Action: Calls the `configure_logging` function imported from `src.utils.logger`. This function is expected to set up log levels, formats, and handlers (e.g., console, file).
        - Referenced Code: `src.utils.logger.configure_logging`
        - Mathematical/Logical Formula: `Execute(configure_logging())`
        - Assessment: Assumed to correctly set up logging as per `src.utils.logger`'s definition.

    II-3.) `try...except` block for `src.utils.config` and GPU libraries (`cudf`, `cupy`)
        - Purpose/Usage: Attempts to load application configuration and GPU-specific libraries, handling potential `ImportError` or other exceptions if they are not available or if `load_config` fails.
        - Action:
            - Calls `load_config()` from `src.utils.config`.
            - Tries to import `cudf` and `cupy`.
            - Sets `GPU_AVAILABLE = True` if both `cudf` and `cupy` are imported successfully.
            - Sets `cudf = None` (though `cupy` is not explicitly set to `None` in the `ImportError` block, which is a minor inconsistency if `cupy` was intended to be used directly elsewhere and checked for `None`) and `GPU_AVAILABLE = False` if `ImportError` occurs for GPU libraries.
            - Logs an exception if any part of this block fails.
        - Referenced Code: `src.utils.config.load_config`, `cudf`, `cupy`, `src.utils.logger.log_statement`
        - Mathematical/Logical Formula:
            `Attempt(Execute(load_config()));`
            `Attempt(Import(cudf) \land Import(cupy) \implies GPU_AVAILABLE := True);`
            `Catch(ImportError \text{ for GPU libs}) \implies (cudf := None \land GPU_AVAILABLE := False);`
            `Catch(Exception e) \implies Log(level='exception', message='Error importing cudf and cupy {e}')`
        - Assessment: Generally robust for optional GPU support and config loading. The `cupy = None` in the except block might be missing if it's intended to be checked like `cudf`. Log statement for the outer exception uses `__file__` as `main_logger`, which is typical. The log level is 'exception' but it doesn't pass `exc_info=True`, which is often good for exceptions.

    II-4.) `try...except` block for `src.data.constants`
        - Purpose/Usage: Attempts to import constants, particularly `LOG_INS`, from a project-specific module.
        - Action:
            - Tries to import everything from `src.data.constants`.
            - Sets `CONSTANTS_AVAILABLE = True` on successful import.
            - Logs an exception and sets `CONSTANTS_AVAILABLE = False` if `ImportError` occurs.
        - Referenced Code: `src.data.constants.*`, `src.utils.logger.log_statement`
        - Mathematical/Logical Formula:
            `Attempt(Import(src.data.constants.*) \implies CONSTANTS_AVAILABLE := True);`
            `Catch(ImportError) \implies (Log(level='exception', message='Importing of constants failed') \land CONSTANTS_AVAILABLE := False)`
        - Assessment: Handles the optional nature of the constants file. The log message `f"{LOG_INS}:ImportError>>..."` assumes `LOG_INS` is already available *before* this import, which is a potential issue if `LOG_INS` itself is defined in `src.data.constants`. A generic log prefix should be used if `LOG_INS` is not yet guaranteed.

    II-5.) `try...except` block for `zstandard as zstd` (second occurrence)
        - Purpose/Usage: This is a redundant import attempt for `zstandard`. It's already imported at the top level.
        - Action: Tries to import `zstandard`. If it fails (which it shouldn't if the first import succeeded), it logs an error and sets `zstd` to `None`.
        - Referenced Code: `zstandard`, `src.utils.logger.log_statement`
        - Mathematical/Logical Formula:
            `Attempt(Import(zstandard as zstd));`
            `Catch(ImportError) \implies (Log(level='warning', message='zstandard not found') \land zstd := None)`
        - Assessment: This block is largely unnecessary due to the earlier import. If the earlier import failed, `zstd` would not be defined, and this block would also fail the import. If the earlier succeeded, this one is just a repeat. The log message here also uses `LOG_INS` which might not be defined if `src.data.constants` failed to import.

III. Class: `DataRepository`
    III-1.) Class Definition and Overall Purpose:
        - `class DataRepository:`
        - Purpose: Manages a collection of data files, tracking their metadata, status, and versions in an index file (typically JSON). It provides functionalities for adding, removing, updating, and querying files, calculating hashes for integrity, compressing/decompressing files, and finding duplicates. It aims to be a centralized handler for datasets within the application.
        - Assessment: The class structure and intended purpose are clear and common for managing local data repositories.

    III-2.) Method: `__init__(self, repository_path, index_filename="repository_index.json", app_state: Optional[Dict[str, Any]] = None, compression_level=3)`
        - **III-2-a.) Purpose and Parameters:**
            - Purpose: Initializes a new `DataRepository` instance. Sets up the repository directory, index file path, loads existing index if available, and initializes a thread lock.
            - Parameters:
                - `self`: Instance of the class.
                - `repository_path` (Union[str, Path]): The root directory for the data repository.
                - `index_filename` (str, optional, default="repository_index.json"): The name of the JSON file used to store the repository index.
                - `app_state` (Optional[Dict[str, Any]], optional, default=None): Application state dictionary, potentially containing configuration or other shared resources. Used by `_determine_processed_path`.
                - `compression_level` (int, optional, default=3): Compression level for zstandard (if used, though it's primarily used in `compress_file` method directly).
        - **III-2-b.) Instance Variable Initialization:**
            - `self.repository_path = Path(repository_path).resolve()`: Stores the absolute, resolved path to the repository directory.
            - `self.index_file_path = self.repository_path / index_filename`: Path to the index file.
            - `self.compression_level = compression_level`: Stores the compression level.
            - `self.app_state = app_state if app_state is not None else {}`: Stores the application state.
            - `self.index_lock = threading.Lock()`: A lock to ensure thread-safe operations on the index file.
            - `self.index = self._load_index()`: Loads the repository index from the file.
        - **III-2-c.) Calls to other methods/functions:**
            - `Path(repository_path).resolve()`: Pathlib method.
            - `self._load_index()`: Calls internal method to load the index.
            - `log_statement()`: For logging.
        - **III-2-d.) Actions taken:**
            1. Resolves and stores the repository path.
            2. Constructs and stores the index file path.
            3. Stores compression level and app_state.
            4. Initializes a `threading.Lock`.
            5. Calls `_load_index()` to populate `self.index`. If `_load_index` fails and returns `None` (e.g., file not found and couldn't be created by `_ensure_index_file` called within `_load_index`), `self.index` might be `None`. The code seems to expect `_load_index` to return an empty dict if the file is new.
            6. Logs initialization.
        - **III-2-e.) Mathematical/Logical Formula:**
            `Initialize(DataRepository_instance) :=`
            `  instance.repository_path := ResolvePath(repository_path)`
            `  instance.index_file_path := instance.repository_path / index_filename`
            `  instance.compression_level := compression_level`
            `  instance.app_state := app_state ?? {}`
            `  instance.index_lock := CreateLock()`
            `  instance.index := instance._load_index()`
            `  Log(level='info', message='Repository initialized at {instance.repository_path}')`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, sets up the basic structure.
            - Failures/Incorrect logic: The error handling in `_load_index` (which creates an empty index if not found) means `self.index` should generally be a dict. If `_ensure_index_file` somehow fails silently in a way that `_load_index` can't recover from to create an empty dict, `self.index` could be problematic. However, the current structure of `_load_index` and `_ensure_index_file` makes this unlikely.
            - Fix/Clarify: Seems okay. The `LOG_INS` constant should be confirmed to be available for the log statement.

    III-3.) Method: `_ensure_index_file(self)`
        - **III-3-a.) Purpose:**
            - Ensures that the repository directory and the index file exist. If the directory or file doesn't exist, it attempts to create them. If the index file is created, it's initialized with an empty JSON object (`{}`).
        - **III-3-b.) Actions taken:**
            1. Creates the repository directory (`self.repository_path`) if it doesn't exist (using `os.makedirs(exist_ok=True)`).
            2. Checks if the index file (`self.index_file_path`) exists.
            3. If the index file does not exist:
                - Logs an info message.
                - Opens the file in write mode (`'w'`).
                - Writes an empty JSON object (`"{}"`) to it.
                - Logs a success message.
            4. Handles `IOError` or `OSError` during file creation, logging an error.
        - **III-3-c.) Mathematical/Logical Formula:**
            `_ensure_index_file(instance) :=`
            `  CreateDirectory(path=instance.repository_path, exist_ok=True)`
            `  If Not Exists(instance.index_file_path) Then`
            `    Log(level='info', message='Index file not found, creating...')`
            `    Try:`
            `      WriteFile(path=instance.index_file_path, content="{}")`
            `      Log(level='info', message='Index file created.')`
            `    Catch (IOError | OSError e):`
            `      Log(level='error', message='Could not create index file: {e}')`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, reliably creates the directory and an initial empty index file if they are missing.
            - Failures/Incorrect logic: None apparent. Uses `exist_ok=True` which is good.
            - Fix/Clarify: Seems robust.

    III-4.) Method: `_load_index(self)`
        - **III-4-a.) Purpose:**
            - Loads the repository index data from the JSON index file into memory (`self.index`). It first ensures the index file exists.
        - **III-4-b.) Actions taken:**
            1. Acquires the `self.index_lock`.
            2. Calls `self._ensure_index_file()` to make sure the file and directory exist.
            3. Tries to open the `self.index_file_path` in read mode (`'r'`).
            4. Parses the JSON content using `json.load()`.
            5. If `FileNotFoundError` occurs (should be rare due to `_ensure_index_file`), logs an error and returns an empty dictionary.
            6. If `json.JSONDecodeError` occurs (e.g., corrupted index file), logs an error and returns an empty dictionary (effectively resetting the index).
            7. If `IOError` or `OSError` occurs, logs an error and returns an empty dictionary.
            8. Logs a debug message with the number of entries loaded.
            9. Releases the `self.index_lock` in a `finally` block.
            10. Returns the loaded index (a dictionary) or an empty dictionary on error.
        - **III-4-c.) Mathematical/Logical Formula:**
            `_load_index(instance) :=`
            `  AcquireLock(instance.index_lock)`
            `  Try:`
            `    instance._ensure_index_file()`
            `    Try:`
            `      index_data := ReadJSON(path=instance.index_file_path)`
            `      Log(level='debug', message='Index loaded with {len(index_data)} entries.')`
            `      Return index_data`
            `    Catch FileNotFoundError e:`
            `      Log(level='error', message='Index file not found: {e}')`
            `      Return {}`
            `    Catch JSONDecodeError e:`
            `      Log(level='warning', message='Error decoding JSON from index file (corrupted?): {e}. Returning empty index.')`
            `      Return {}`
            `    Catch (IOError | OSError e):`
            `      Log(level='error', message='Could not read index file: {e}')`
            `      Return {}`
            `  Finally:`
            `    ReleaseLock(instance.index_lock)`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, loads the index safely.
            - Failures/Incorrect logic: Returning an empty dictionary on `JSONDecodeError` means a corrupted index leads to data loss (metadata is lost, not the files themselves). This might be desired behavior (start fresh) or undesired. A backup/recovery mechanism for a corrupted index could be an enhancement (see `backup_repository_index`).
            - Fix/Clarify: The behavior for a corrupted index is a design choice. Logging it as a warning is reasonable if resetting is the intended fallback.

    III-5.) Method: `_save_index(self)`
        - **III-5-a.) Purpose:**
            - Saves the current in-memory repository index (`self.index`) to the JSON index file.
        - **III-5-b.) Actions taken:**
            1. Acquires the `self.index_lock`.
            2. Tries to open the `self.index_file_path` in write mode (`'w'`).
            3. Serializes `self.index` to JSON using `json.dump()` with an indent for readability.
            4. If `IOError` or `OSError` occurs, logs an error.
            5. Logs a debug message.
            6. Releases the `self.index_lock` in a `finally` block.
        - **III-5-c.) Mathematical/Logical Formula:**
            `_save_index(instance) :=`
            `  AcquireLock(instance.index_lock)`
            `  Try:`
            `    WriteJSON(path=instance.index_file_path, data=instance.index, indent=4)`
            `    Log(level='debug', message='Index saved to {instance.index_file_path}')`
            `  Catch (IOError | OSError e):`
            `    Log(level='error', message='Could not save index file: {e}')`
            `  Finally:`
            `    ReleaseLock(instance.index_lock)`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, saves the index thread-safely.
            - Failures/Incorrect logic: If `self.index` is `None` for some unexpected reason, `json.dump` would fail. Given `_load_index`'s behavior, this is unlikely.
            - Fix/Clarify: Robust. Using `indent=4` is good for human readability of the index file.

    III-6.) Method: `_get_relative_path(self, file_path: Union[str, Path]) -> str`
        - **III-6-a.) Purpose and Parameters:**
            - Purpose: Converts an absolute file path or a `Path` object to a path string relative to the `self.repository_path`.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): The file path to make relative.
        - **III-6-b.) Actions taken:**
            1. Converts `file_path` to an absolute `Path` object.
            2. Tries to compute the path relative to `self.repository_path` using `Path.relative_to()`.
            3. If `ValueError` occurs (e.g., `file_path` is not under `self.repository_path`), logs an error and re-raises the exception. This is a critical issue if a file outside the repo is attempted to be processed as if it's inside.
            4. Returns the relative path as a string.
        - **III-6-c.) Mathematical/Logical Formula:**
            `_get_relative_path(instance, file_path_input) :=`
            `  abs_file_path := Absolute(Path(file_path_input))`
            `  Try:`
            `    relative_path := abs_file_path.relative_to(instance.repository_path)`
            `    Return AsString(relative_path)`
            `  Catch ValueError e:`
            `    Log(level='error', message='File {abs_file_path} is not within the repository path {instance.repository_path}. {e}')`
            `    Throw e`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: Re-raising `ValueError` is appropriate as this indicates a fundamental problem with the input `file_path` if it's expected to be within the repository.
            - Fix/Clarify: Correct. The logging helps diagnose.

    III-7.) Method: `_get_absolute_path(self, relative_file_path: Union[str, Path]) -> Path`
        - **III-7-a.) Purpose and Parameters:**
            - Purpose: Converts a relative file path (string or `Path` object, assumed to be relative to the repository root) to an absolute `Path` object.
            - Parameters:
                - `self`: Instance of the class.
                - `relative_file_path` (Union[str, Path]): The relative path.
        - **III-7-b.) Actions taken:**
            1. Joins `self.repository_path` with `relative_file_path`.
            2. Resolves the joined path to get an absolute, canonical path.
        - **III-7-c.) Mathematical/Logical Formula:**
            `_get_absolute_path(instance, relative_path_input) :=`
            `  abs_path := ResolvePath(instance.repository_path / Path(relative_path_input))`
            `  Return abs_path`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent.
            - Fix/Clarify: Correct and straightforward.

    III-8.) Method: `_calculate_hashes(self, file_path: Union[str, Path], file_size: int) -> Dict[str, str]`
        - **III-8-a.) Purpose and Parameters:**
            - Purpose: Calculates MD5 and SHA256 hashes for the content of a given file.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): The path to the file to be hashed.
                - `file_size` (int): The size of the file (passed in, likely to avoid a `stat` call if already known).
        - **III-8-b.) Actions taken:**
            1. Calls the `hash_file` function (imported from `src.utils.hashing`) with the file path and `file_size`. This external function is expected to return a dictionary of hashes.
            2. Returns the dictionary of hashes (e.g., `{'md5': '...', 'sha256': '...'}`).
        - **III-8-c.) Mathematical/Logical Formula:**
            `_calculate_hashes(instance, file_path_input, file_size_input) :=`
            `  hashes_dict := hash_file(filepath=file_path_input, current_size=file_size_input) (from src.utils.hashing)`
            `  Return hashes_dict`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, by delegating to `src.utils.hashing.hash_file`.
            - Failures/Incorrect logic: Depends entirely on the correctness of the external `hash_file` function. Assumed to be correct.
            - Fix/Clarify: The name `current_size` in the call to `hash_file` implies it might be used for change detection or optimization within `hash_file`, though here it's just passed through.

    III-9.) Method: `add_file(self, file_path: Union[str, Path], metadata: Optional[Dict[str, Any]] = None, status: str = "active", app_state: Optional[Dict[str, Any]] = None) -> bool`
        - **III-9-a.) Purpose and Parameters:**
            - Purpose: Adds a file to the repository. This involves calculating its hash, generating metadata, and updating the index file.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): Path to the file to add.
                - `metadata` (Optional[Dict[str, Any]], optional): Additional user-supplied metadata for the file.
                - `status` (str, optional, default="active"): Initial status of the file.
                - `app_state` (Optional[Dict[str, Any]], optional): Application state, used if `_determine_processed_path` needs to be called (though not directly in this version of `add_file`).
        - **III-9-b.) Core logic:**
            1. Converts `file_path` to an absolute `Path` object.
            2. Logs memory usage if `PSUTIL_AVAILABLE`.
            3. Checks if the file exists; returns `False` and logs error if not.
            4. Gets the relative path of the file using `_get_relative_path`. If this fails (file not in repo), it returns `False`.
            5. Gets file size using `os.path.getsize()`.
            6. Calculates hashes using `_calculate_hashes()`.
            7. Prepares the file entry dictionary:
                - `absolute_path`: Stored as string.
                - `size`: File size in bytes.
                - `hashes`: Dictionary of calculated hashes.
                - `date_added`: Current UTC timestamp.
                - `date_modified`: File's last modification timestamp (from `os.path.getmtime`).
                - `status`: Provided status.
                - `metadata`: User-provided metadata merged with a default empty dict.
                - `version`: Initialized to 1 (simple versioning).
                - `history`: List to track changes, initialized with current state.
            8. Checks if the file (by relative path) is already in the index:
                - If yes, checks if content (SHA256 hash) or modification date has changed.
                    - If changed, increments version, updates relevant fields, and appends to history.
                    - If not changed, logs that file is unchanged and returns `True`.
                - If no (new file), adds the entry.
            9. Saves the updated index using `_save_index()`.
            10. Logs success and returns `True`.
            11. Handles `Exception` during the process, logs it, and returns `False`.
        - **III-9-c.) Calls to other methods/functions:**
            - `Path()`, `Path.resolve()`, `Path.exists()`, `str()`
            - `self._log_memory_usage()`
            - `self._get_relative_path()`
            - `os.path.getsize()`
            - `self._calculate_hashes()`
            - `dt.now(timezone.utc).isoformat()`
            - `dt.fromtimestamp(os.path.getmtime(absolute_fp), timezone.utc).isoformat()`
            - `self._save_index()`
            - `log_statement()`
        - **III-9-d.) Error handling:**
            - Checks for file existence.
            - Catches exception from `_get_relative_path` if file not in repository structure.
            - General `Exception` catch-all for other errors during processing.
        - **III-9-e.) Mathematical/Logical Formula (Simplified):**
            `add_file(instance, file_path_input, metadata_input, status_input, app_state_input) :=`
            `  abs_fp := ResolvePath(Path(file_path_input))`
            `  If Not Exists(abs_fp) Then Return False`
            `  rel_fp_str := instance._get_relative_path(abs_fp)`
            `  If rel_fp_str is Error Then Return False`
            `  file_size := GetSize(abs_fp)`
            `  hashes := instance._calculate_hashes(abs_fp, file_size)`
            `  entry_data := { path: rel_fp_str, size: file_size, hashes: hashes, ... }`
            `  If rel_fp_str \in instance.index Then`
            `    If HasChanged(instance.index[rel_fp_str], entry_data) Then`
            `      UpdateEntry(instance.index[rel_fp_str], entry_data)`
            `    Else`
            `      LogInfo("File unchanged")`
            `      Return True`
            `  Else`
            `    instance.index[rel_fp_str] := entry_data`
            `  instance._save_index()`
            `  Return True`
            `Catch Exception e: LogError(e); Return False`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, a comprehensive way to add/update files in the index.
            - Failures/Incorrect logic:
                - The `app_state` parameter is accepted but not directly used in this method in the provided code, suggesting it might be for a feature not fully implemented here or intended for use by overriding classes/extensions. It *is* used by `_determine_processed_path` which is not called by `add_file`.
                - The "simple versioning" (incrementing a number and storing history) is basic. For more robust versioning, especially for ML models/data, Git integration (the ultimate goal) would be far superior.
                - Storing the absolute path (`entry_data['absolute_path'] = str(absolute_fp)`) in the index in addition to using the relative path as the key is a bit redundant if `_get_absolute_path` is always used for retrieval. It could lead to inconsistencies if the repository is moved, though `repository_path` is resolved at init. Typically, only relative paths are stored in such indexes.
            - Fix/Clarify:
                - Consider if `absolute_path` in the index entry is truly necessary or if `self.repository_path` + relative key is always sufficient. If kept, ensure it's clear how it's used and maintained if the repo moves (though `repository_path` in `__init__` being resolved helps).
                - The versioning is a placeholder for what Git will eventually provide.

    III-10.) Method: `add_files_threaded(self, file_paths: List[Union[str, Path]], app_state: Optional[Dict[str, Any]] = None, num_threads: Optional[int] = None) -> Dict[str, bool]`
        - **III-10-a.) Purpose and Parameters:**
            - Purpose: Adds multiple files to the repository concurrently using a thread pool.
            - Parameters:
                - `self`: Instance of the class.
                - `file_paths` (List[Union[str, Path]]): A list of file paths to add.
                - `app_state` (Optional[Dict[str, Any]], optional): Application state, passed to `add_file`.
                - `num_threads` (Optional[int], optional): Number of threads to use. If None, uses `_get_max_workers()`.
        - **III-10-b.) Concurrency using `ThreadPoolExecutor`:**
            1. Determines the number of worker threads (`max_workers`).
            2. Initializes a `ThreadPoolExecutor`.
            3. Submits each file path to the executor to call `self.add_file`.
            4. Uses `tqdm` to show progress as futures complete.
            5. Collects results (success/failure boolean) for each file.
        - **III-10-c.) Calls to `add_file`:** Each file is processed by `self.add_file` in a separate thread.
        - **III-10-d.) Mathematical/Logical Formula:**
            `add_files_threaded(instance, list_of_paths, app_state_input, num_threads_input) :=`
            `  workers := num_threads_input ?? _get_max_workers()`
            `  results := {}`
            `  With ThreadPoolExecutor(max_workers=workers) as executor:`
            `    futures := {executor.submit(instance.add_file, fp, app_state=app_state_input): fp for fp in list_of_paths}`
            `    For future in AsCompleted(futures_with_tqdm_progress):`
            `      results[futures[future]] := future.result()`
            `  Return results`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, provides concurrent processing for adding multiple files.
            - Failures/Incorrect logic: `self.add_file` involves disk I/O and CPU-bound hash calculations. Threading is suitable for I/O-bound parts. The GIL might limit true parallelism for CPU-bound parts in CPython, but offloading I/O wait times is beneficial. The `index_lock` in `_save_index` and `_load_index` (called by `add_file`) is crucial here and correctly serializes access to the index file itself, preventing race conditions on the index.
            - Fix/Clarify: Seems okay for its purpose. The `app_state` is passed through.

    III-11.) Method: `update_file_status(self, file_path: Union[str, Path], new_status: str) -> bool`
        - **III-11-a.) Purpose and Parameters:**
            - Purpose: Updates the status of an existing file in the repository index.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): The path of the file (relative or absolute) whose status needs to be updated.
                - `new_status` (str): The new status to assign to the file (e.g., "archived", "processed").
        - **III-11-b.) Actions:**
            1. Tries to get the relative path of the file using `_get_relative_path`. If `file_path` is already relative, this might involve converting to absolute then back to relative, or it might handle it gracefully. It is designed to ensure a consistent key for the index.
            2. If `_get_relative_path` fails (e.g., file not in repo structure, or other `ValueError`), logs an error and returns `False`.
            3. Checks if the relative file path exists as a key in `self.index`.
            4. If the file is found in the index:
                - Updates the 'status' field of the file's entry.
                - Updates the 'date_modified' field to the current UTC time (reflecting the metadata change).
                - Appends a history record to the file's entry detailing the status change.
                - Calls `self._save_index()` to persist the changes.
                - Logs the status update and returns `True`.
            5. If the file is not found in the index, logs a warning and returns `False`.
        - **III-11-c.) Calls to other methods/functions:**
            - `self._get_relative_path()`
            - `dt.now(timezone.utc).isoformat()`
            - `self._save_index()`
            - `log_statement()`
        - **III-11-d.) Mathematical/Logical Formula:**
            `update_file_status(instance, file_path_input, new_status_input) :=`
            `  Try:`
            `    rel_fp_str := instance._get_relative_path(file_path_input)`
            `  Catch ValueError e:`
            `    Log(level='error', message='Error getting relative path for {file_path_input}: {e}')`
            `    Return False`
            `  If rel_fp_str \in instance.index Then`
            `    entry := instance.index[rel_fp_str]`
            `    old_status := entry['status']`
            `    entry['status'] := new_status_input`
            `    entry['date_modified'] := GetCurrentUTCTimestampISO()`
            `    AppendToHistory(entry, action='status_update', from=old_status, to=new_status_input, timestamp=GetCurrentUTCTimestampISO())`
            `    instance._save_index()`
            `    Log(level='info', message='Status updated for {rel_fp_str} to {new_status_input}')`
            `    Return True`
            `  Else`
            `    Log(level='warning', message='File {rel_fp_str} not found in index for status update.')`
            `    Return False`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, it correctly updates the status and modification time for a file entry and records history.
            - Failures/Incorrect logic: The use of `_get_relative_path` assumes the `file_path` provided might be absolute and needs conversion. If a relative path is given that's not syntactically "within" the repo path (e.g. `../somefile`), `_get_relative_path` as implemented might raise an error. It's generally robust for paths known to be within the repo.
            - Fix/Clarify: The logic seems sound for its stated purpose.

    III-12.) Method: `get_file_metadata(self, file_path: Union[str, Path]) -> Optional[Dict[str, Any]]`
        - **III-12-a.) Purpose and Parameters:**
            - Purpose: Retrieves the metadata dictionary for a specific file from the repository index.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): The path of the file (relative or absolute) whose metadata is requested.
        - **III-12-b.) Actions:**
            1. Tries to get the relative path string using `self._get_relative_path()`.
            2. If `_get_relative_path` fails, logs an error and returns `None`.
            3. If the relative path is found as a key in `self.index`, returns the corresponding metadata dictionary.
            4. If the file is not found, logs a warning and returns `None`.
        - **III-12-c.) Calls to other methods/functions:**
            - `self._get_relative_path()`
            - `log_statement()`
        - **III-12-d.) Mathematical/Logical Formula:**
            `get_file_metadata(instance, file_path_input) :=`
            `  Try:`
            `    rel_fp_str := instance._get_relative_path(file_path_input)`
            `  Catch ValueError e:`
            `    Log(level='error', message='Error getting relative path for {file_path_input} for metadata retrieval: {e}')`
            `    Return None`
            `  If rel_fp_str \in instance.index Then`
            `    Return instance.index[rel_fp_str]`
            `  Else`
            `    Log(level='warning', message='File {rel_fp_str} not found in index for metadata retrieval.')`
            `    Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, it retrieves metadata for a given file.
            - Failures/Incorrect logic: None apparent. Handles cases where the file is not in the index or path conversion fails.
            - Fix/Clarify: Clear and correct.

    III-13.) Method: `get_files_by_status(self, status_filter: Union[str, List[str]]) -> List[Path]`
        - **III-13-a.) Purpose and Parameters:**
            - Purpose: Retrieves a list of absolute file paths from the repository that match a given status or list of statuses.
            - Parameters:
                - `self`: Instance of the class.
                - `status_filter` (Union[str, List[str]]): A single status string or a list of status strings to filter by.
        - **III-13-b.) Actions:**
            1. Initializes an empty list `matching_files`.
            2. If `status_filter` is a single string, converts it to a list containing that string.
            3. Iterates through all file entries (`rel_path`, `metadata_dict`) in `self.index.items()`.
            4. For each file, checks if its 'status' field is in the `status_filter` list.
            5. If the status matches, converts the relative path (`rel_path`) to an absolute path using `self._get_absolute_path()` and appends it to `matching_files`.
            6. Logs the number of files found.
            7. Returns the `matching_files` list.
        - **III-13-c.) Calls to other methods/functions:**
            - `self._get_absolute_path()`
            - `log_statement()`
        - **III-13-d.) Mathematical/Logical Formula:**
            `get_files_by_status(instance, status_filter_input) :=`
            `  filters := ToList(status_filter_input)`
            `  matched_paths := []`
            `  For (rel_path, meta) in instance.index.items():`
            `    If meta['status'] \in filters Then`
            `      abs_path := instance._get_absolute_path(rel_path)`
            `      AddToList(matched_paths, abs_path)`
            `  Log(level='debug', message='Found {len(matched_paths)} files with status(es) {filters}')`
            `  Return matched_paths`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent. Correctly handles single or list of statuses.
            - Fix/Clarify: Efficient and clear.

    III-14.) Method: `get_all_files(self) -> List[Path]`
        - **III-14-a.) Purpose:**
            - Retrieves a list of absolute file paths for all files currently tracked in the repository index, regardless of their status (unless implicitly filtered by not being in the index at all).
        - **III-14-b.) Actions:**
            1. Initializes an empty list `all_repo_files`.
            2. Iterates through all relative paths (keys) in `self.index`.
            3. For each `rel_path`, converts it to an absolute path using `self._get_absolute_path()` and appends it to `all_repo_files`.
            4. Logs the total number of files retrieved.
            5. Returns the `all_repo_files` list.
        - **III-14-c.) Calls to other methods/functions:**
            - `self._get_absolute_path()`
            - `log_statement()`
        - **III-14-d.) Mathematical/Logical Formula:**
            `get_all_files(instance) :=`
            `  all_paths := []`
            `  For rel_path in instance.index.keys():`
            `    abs_path := instance._get_absolute_path(rel_path)`
            `    AddToList(all_paths, abs_path)`
            `  Log(level='debug', message='Retrieved {len(all_paths)} total files from repository index.')`
            `  Return all_paths`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent.
            - Fix/Clarify: Simple and effective.

    III-15.) Method: `remove_file(self, file_path: Union[str, Path], permanent: bool = False) -> bool`
        - **III-15-a.) Purpose and Parameters:**
            - Purpose: Removes a file's entry from the repository index. Optionally, it can also delete the physical file from the disk.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): The path of the file to remove (relative or absolute).
                - `permanent` (bool, optional, default=False): If `True`, deletes the physical file from disk. Otherwise, only marks it as 'removed' or deletes its entry.
        - **III-15-b.) Actions:**
            1. Tries to get the relative path string using `self._get_relative_path()`.
            2. If `_get_relative_path` fails, logs an error and returns `False`.
            3. Checks if the file (by relative path) exists in `self.index`.
            4. If the file is found in the index:
                - If `permanent` is `True`:
                    - Gets the absolute path using `self._get_absolute_path()`.
                    - Tries to delete the physical file using `os.remove()`.
                    - If `os.remove()` fails (e.g., `FileNotFoundError`, `OSError`), logs an error and returns `False` (without altering the index).
                    - If physical deletion is successful, removes the entry from `self.index` using `del`.
                    - Logs the permanent removal.
                - If `permanent` is `False`:
                    - Sets the file's status to 'removed'. (The code actually uses `del self.index[rel_fp_str]`, which is a permanent removal from the index, not just a status update. This contradicts the comment "marks it as 'removed'").
                    - Logs that the entry was removed from the index.
                - Calls `self._save_index()` to persist changes.
                - Returns `True`.
            5. If the file is not found in the index, logs a warning and returns `False`.
        - **III-15-c.) Calls to other methods/functions:**
            - `self._get_relative_path()`
            - `self._get_absolute_path()` (if `permanent`)
            - `os.remove()` (if `permanent`)
            - `self._save_index()`
            - `log_statement()`
        - **III-15-d.) Mathematical/Logical Formula:**
            `remove_file(instance, file_path_input, permanent_flag) :=`
            `  Try:`
            `    rel_fp_str := instance._get_relative_path(file_path_input)`
            `  Catch ValueError e:`
            `    Log(level='error', message='Error getting relative path for {file_path_input} for removal: {e}')`
            `    Return False`
            `  If rel_fp_str \in instance.index Then`
            `    If permanent_flag Then`
            `      abs_fp := instance._get_absolute_path(rel_fp_str)`
            `      Try:`
            `        DeleteFile(abs_fp)`
            `        Log(level='info', message='Permanently deleted physical file: {abs_fp}')`
            `        DeleteKey(instance.index, rel_fp_str)`
            `      Catch (FileNotFoundError | OSError e):`
            `        Log(level='error', message='Error deleting physical file {abs_fp}: {e}')`
            `        Return False`
            `    Else`
            `      DeleteKey(instance.index, rel_fp_str)  // Current behavior`
            `      // Alternative: instance.index[rel_fp_str]['status'] := 'removed'`
            `      Log(level='info', message='Removed entry {rel_fp_str} from index.')`
            `    instance._save_index()`
            `    Return True`
            `  Else`
            `    Log(level='warning', message='File {rel_fp_str} not found in index for removal.')`
            `    Return False`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Mostly.
            - Failures/Incorrect logic:
                - The behavior for `permanent=False` is `del self.index[rel_fp_str]`, which removes the entry entirely from the index. This contradicts the common understanding of a "soft delete" or marking as "removed" where the entry might still exist for historical purposes or potential recovery. The comment "marks it as 'removed'" suggests a different intent than the implementation. If the goal is to truly remove it from tracking, the current code is correct. If the goal is to mark it but keep a record, it should update the status.
            - Fix/Clarify:
                - If `permanent=False` should mean "mark as removed but keep in index", then instead of `del self.index[rel_fp_str]`, it should be `self.index[rel_fp_str]['status'] = 'removed'` (and potentially update 'date_modified' and history similar to `update_file_status`).
                - If `permanent=False` is intended to mean "remove from index but don't touch disk file", the current code is correct, but the comment should be updated.

    III-16.) Method: `verify_checksums(self, file_path: Optional[Union[str, Path]] = None) -> Dict[str, str]`
        - **III-16-a.) Purpose and Parameters:**
            - Purpose: Verifies the integrity of files in the repository by recalculating their checksums (MD5, SHA256) and comparing them against the stored checksums in the index. Can verify a single file or all files.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Optional[Union[str, Path]], optional): If provided, verifies only this specific file. If `None`, verifies all files in the index.
        - **III-16-b.) Actions:**
            1. Initializes an empty dictionary `verification_results`.
            2. Determines the list of files to check:
                - If `file_path` is provided:
                    - Tries to get the relative path. If error, logs it and returns `{'error': 'File path processing error'}`.
                    - If the file is not in the index, logs it and returns `{'error': 'File not in index'}`.
                    - Sets `files_to_check` to a list containing this single file's entry.
                - If `file_path` is `None`:
                    - Sets `files_to_check` to all items in `self.index.items()`.
            3. Iterates through `files_to_check` (using `tqdm` for progress if multiple files):
                - For each `rel_path`, `entry_data`:
                    - Gets the absolute path using `self._get_absolute_path()`.
                    - Checks if the physical file exists. If not, records 'MISSING' status and continues.
                    - Gets the stored hashes from `entry_data`.
                    - Recalculates current hashes using `self._calculate_hashes()`.
                    - Compares stored SHA256 with current SHA256. Records 'OK' or 'MISMATCH'.
                    - If MD5 is present in stored hashes, compares it too. Records 'OK (MD5 also)', 'MISMATCH (MD5 OK)' etc. based on SHA256 and MD5 agreement.
            4. Logs a summary of verification.
            5. Returns the `verification_results` dictionary.
        - **III-16-c.) Calls to other methods/functions:**
            - `self._get_relative_path()`
            - `self._get_absolute_path()`
            - `Path.exists()`
            - `os.path.getsize()` (implicitly by `_calculate_hashes`)
            - `self._calculate_hashes()`
            - `tqdm()`
            - `log_statement()`
        - **III-16-d.) Mathematical/Logical Formula (Simplified):**
            `verify_checksums(instance, opt_file_path) :=`
            `  results := {}`
            `  files_to_process := DetermineTargetFiles(opt_file_path, instance.index)`
            `  If ErrorInTargetFiles Then Return {error: ...}`
            `  For (rel_path, entry) in ProgressBar(files_to_process):`
            `    abs_path := instance._get_absolute_path(rel_path)`
            `    If Not Exists(abs_path) Then results[rel_path] := 'MISSING'; Continue`
            `    stored_hashes := entry['hashes']`
            `    current_hashes := instance._calculate_hashes(abs_path, GetSize(abs_path))`
            `    If stored_hashes['sha256'] == current_hashes['sha256'] Then`
            `      results[rel_path] := 'OK'`
            `      // Further check MD5 if present`
            `    Else`
            `      results[rel_path] := 'MISMATCH'`
            `  LogSummary(results)`
            `  Return results`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, provides a good way to check file integrity.
            - Failures/Incorrect logic:
                - The logic for reporting MD5 status alongside SHA256 is a bit complex in the `if/elif/else` chain and could be simplified or made clearer. The primary check is SHA256.
                - The `file_size` is fetched again for `_calculate_hashes` even though it's stored in `entry_data`. Could potentially use the stored size if the file's existence is confirmed.
            - Fix/Clarify: The method is largely functional. Stored file size could be passed to `_calculate_hashes` if desired, though recalculating based on current file on disk is safer for integrity checks. The result reporting logic for combined hash statuses could be streamlined.

    III-17.) Method: `compress_file(self, file_path: Union[str, Path], output_path: Optional[Union[str, Path]] = None, remove_original: bool = False) -> Optional[Path]`
        - **III-17-a.) Purpose and Parameters:**
            - Purpose: Compresses a given file using Zstandard compression.
            - Parameters:
                - `self`: Instance of the class.
                - `file_path` (Union[str, Path]): Path to the file to compress.
                - `output_path` (Optional[Union[str, Path]], optional): Path to save the compressed file. If `None`, appends `.zst` to the original filename in the same directory.
                - `remove_original` (bool, optional, default=False): If `True`, deletes the original file after successful compression.
        - **III-17-b.) Actions:**
            1. Checks if `zstd` (Zstandard library) is available. If not, logs an error and returns `None`.
            2. Converts `file_path` to an absolute `Path` object.
            3. If `file_path` does not exist, logs error and returns `None`.
            4. Determines the `actual_output_path`:
                - If `output_path` is provided, uses it.
                - Else, creates it by appending `.zst` to `file_path.name` in `file_path.parent`.
            5. Ensures the output directory exists using `os.makedirs`.
            6. Creates a `zstd.ZstdCompressor` with `self.compression_level`.
            7. Opens the input file in binary read mode (`'rb'`) and output file in binary write mode (`'wb'`).
            8. Uses `shutil.copyfileobj` with `compressor.stream_writer(outfile)` to perform streaming compression, which is memory efficient.
            9. If compression is successful and `remove_original` is `True`, deletes the original file using `os.remove()`.
            10. Logs success and returns the `actual_output_path`.
            11. Handles `Exception` during the process, logs it, and returns `None`.
        - **III-17-c.) Calls to other methods/functions:**
            - `Path()`
            - `Path.exists()`, `Path.parent`, `Path.name`, `Path.with_suffix()` (implicitly, or could be used)
            - `os.makedirs()`
            - `zstd.ZstdCompressor()`, `compressor.stream_writer()`
            - `shutil.copyfileobj()`
            - `os.remove()` (if `remove_original`)
            - `log_statement()`
        - **III-17-d.) Mathematical/Logical Formula:**
            `compress_file(instance, fp_in, opt_fp_out, remove_orig_flag) :=`
            `  If Not ZstdAvailable() Then Return None`
            `  abs_fp_in := Absolute(Path(fp_in))`
            `  If Not Exists(abs_fp_in) Then Return None`
            `  abs_fp_out := DetermineOutputPath(abs_fp_in, opt_fp_out, ".zst")`
            `  CreateDirectory(Parent(abs_fp_out))`
            `  Try:`
            `    CompressStream(source=abs_fp_in, dest=abs_fp_out, level=instance.compression_level)`
            `    If remove_orig_flag Then DeleteFile(abs_fp_in)`
            `    Log(level='info', message='Compressed {abs_fp_in} to {abs_fp_out}')`
            `    Return abs_fp_out`
            `  Catch Exception e: LogError(e); Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: Uses `shutil.copyfileobj` with `compressor.stream_writer`, which is good. The `with_suffix` method of `Path` could simplify output path generation if `output_path` is None: `actual_output_path = file_path.with_suffix(file_path.suffix + '.zst')`. Current logic for default output path is also fine.
            - Fix/Clarify: Solid implementation.

    III-18.) Method: `decompress_file(self, compressed_file_path: Union[str, Path], output_path: Optional[Union[str, Path]] = None, remove_original: bool = False) -> Optional[Path]`
        - **III-18-a.) Purpose and Parameters:**
            - Purpose: Decompresses a Zstandard compressed file.
            - Parameters:
                - `self`: Instance of the class.
                - `compressed_file_path` (Union[str, Path]): Path to the compressed file (usually ending in `.zst`).
                - `output_path` (Optional[Union[str, Path]], optional): Path to save the decompressed file. If `None`, removes `.zst` (if present) from the original filename.
                - `remove_original` (bool, optional, default=False): If `True`, deletes the original compressed file after successful decompression.
        - **III-18-b.) Actions:**
            1. Checks if `zstd` is available. If not, logs error and returns `None`.
            2. Converts `compressed_file_path` to an absolute `Path` object.
            3. If `compressed_file_path` does not exist, logs error and returns `None`.
            4. Determines the `actual_output_path`:
                - If `output_path` is provided, uses it.
                - Else, if `compressed_file_path.name` ends with `.zst`, removes it. Otherwise, appends `_decompressed`.
            5. Ensures the output directory exists using `os.makedirs`.
            6. Creates a `zstd.ZstdDecompressor`.
            7. Opens the compressed input file (`'rb'`) and output file (`'wb'`).
            8. Uses `shutil.copyfileobj` with `decompressor.stream_reader(infile)` to perform streaming decompression.
            9. If decompression is successful and `remove_original` is `True`, deletes the original compressed file.
            10. Logs success and returns the `actual_output_path`.
            11. Handles `Exception`, logs it, and returns `None`.
        - **III-18-c.) Calls to other methods/functions:**
            - `Path()`
            - `Path.exists()`, `Path.parent`, `Path.name`, `Path.stem` (could be used)
            - `os.makedirs()`
            - `zstd.ZstdDecompressor()`, `decompressor.stream_reader()`
            - `shutil.copyfileobj()`
            - `os.remove()` (if `remove_original`)
            - `log_statement()`
        - **III-18-d.) Mathematical/Logical Formula:**
            `decompress_file(instance, fp_in_comp, opt_fp_out, remove_orig_flag) :=`
            `  If Not ZstdAvailable() Then Return None`
            `  abs_fp_in_comp := Absolute(Path(fp_in_comp))`
            `  If Not Exists(abs_fp_in_comp) Then Return None`
            `  abs_fp_out := DetermineDecompressedOutputPath(abs_fp_in_comp, opt_fp_out)`
            `  CreateDirectory(Parent(abs_fp_out))`
            `  Try:`
            `    DecompressStream(source=abs_fp_in_comp, dest=abs_fp_out)`
            `    If remove_orig_flag Then DeleteFile(abs_fp_in_comp)`
            `    Log(level='info', message='Decompressed {abs_fp_in_comp} to {abs_fp_out}')`
            `    Return abs_fp_out`
            `  Catch Exception e: LogError(e); Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: Default output path logic is reasonable. `Path.stem` could be useful if the original file had multiple extensions before `.zst` (e.g., `archive.tar.zst` -> `archive.tar`). Current logic removes only the final `.zst`.
            - Fix/Clarify: Robust.

    III-19.) Method: `backup_repository_index(self, backup_dir: Optional[Union[str, Path]] = None) -> Optional[Path]`
        - **III-19-a.) Purpose and Parameters:**
            - Purpose: Creates a timestamped backup of the current repository index file.
            - Parameters:
                - `self`: Instance of the class.
                - `backup_dir` (Optional[Union[str, Path]], optional): The directory where the backup file should be saved. If `None`, saves it in a 'backups' subdirectory within the main repository directory.
        - **III-19-b.) Actions:**
            1. Determines the backup directory:
                - If `backup_dir` is provided, uses it.
                - Else, creates it as `self.repository_path / "backups"`.
            2. Creates the backup directory if it doesn't exist using `os.makedirs`.
            3. Generates a timestamp string (e.g., `YYYYMMDD_HHMMSS`).
            4. Constructs the backup filename: `index_backup_{timestamp}.json`.
            5. Constructs the full backup file path.
            6. Tries to copy the current index file (`self.index_file_path`) to the backup file path using `shutil.copy2` (preserves metadata).
            7. Logs success and returns the backup file path.
            8. Handles `Exception`, logs it, and returns `None`.
        - **III-19-c.) Calls to other methods/functions:**
            - `Path()`
            - `os.makedirs()`
            - `dt.now().strftime()`
            - `shutil.copy2()`
            - `log_statement()`
        - **III-19-d.) Mathematical/Logical Formula:**
            `backup_repository_index(instance, opt_backup_dir) :=`
            `  backup_target_dir := opt_backup_dir ?? (instance.repository_path / "backups")`
            `  CreateDirectory(backup_target_dir, exist_ok=True)`
            `  timestamp_str := FormatTimestamp(GetCurrentTime(), "%Y%m%d_%H%M%S")`
            `  backup_filename := "index_backup_" + timestamp_str + ".json"`
            `  backup_filepath := backup_target_dir / backup_filename`
            `  Try:`
            `    CopyFile(source=instance.index_file_path, dest=backup_filepath, preserve_metadata=True)`
            `    Log(level='info', message='Repository index backed up to {backup_filepath}')`
            `    Return backup_filepath`
            `  Catch Exception e: LogError(e); Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent. `shutil.copy2` is a good choice.
            - Fix/Clarify: Correct and useful.

    III-20.) Method: `get_repository_summary(self) -> Dict[str, Any]`
        - **III-20-a.) Purpose:**
            - Provides a summary of the repository, including total number of files, total size, and counts of files by status.
        - **III-20-b.) Actions:**
            1. Initializes `total_files = 0`, `total_size = 0`.
            2. Initializes `status_counts = collections.defaultdict(int)`.
            3. Iterates through all entries in `self.index.values()`:
                - Increments `total_files`.
                - Adds file's 'size' to `total_size`.
                - Increments the count for the file's 'status' in `status_counts`.
            4. Creates a summary dictionary containing `total_files`, `total_size` (human-readable might be nice too, but raw bytes is fine), and `status_counts`.
            5. Logs a debug message about generating the summary.
            6. Returns the summary dictionary.
        - **III-20-c.) Calls to other methods/functions:**
            - `collections.defaultdict()`
            - `log_statement()`
        - **III-20-d.) Mathematical/Logical Formula:**
            `get_repository_summary(instance) :=`
            `  summary := { total_files: 0, total_size: 0, status_counts: defaultdict(int) }`
            `  For entry in instance.index.values():`
            `    summary.total_files += 1`
            `    summary.total_size += entry['size']`
            `    summary.status_counts[entry['status']] += 1`
            `  Log(level='debug', message='Generated repository summary.')`
            `  Return summary`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent. Uses `defaultdict` effectively.
            - Fix/Clarify: Correct. Could add a human-readable size string to the summary as an enhancement.

    III-21.) Method: `@staticmethod \n _log_memory_usage(context: str = "")`
        - **III-21-a.) Purpose and Parameters:**
            - Purpose: A static utility method to log the current Python process's memory usage if `psutil` is available.
            - Parameters:
                - `context` (str, optional, default=""): A string to provide context for the memory log entry.
        - **III-21-b.) Actions:**
            1. Checks the global `PSUTIL_AVAILABLE` flag.
            2. If `psutil` is available:
                - Gets the current process using `psutil.Process()`.
                - Gets memory info using `process.memory_info()`.
                - Logs the Resident Set Size (RSS) and Virtual Memory Size (VMS) in MB.
            3. If `psutil` is not available, does nothing (or could log that psutil is unavailable for memory logging).
        - **III-21-c.) Calls to other methods/functions:**
            - `psutil.Process()` (if `PSUTIL_AVAILABLE`)
            - `process.memory_info()` (if `PSUTIL_AVAILABLE`)
            - `log_statement()` (if `PSUTIL_AVAILABLE`)
        - **III-21-d.) Mathematical/Logical Formula:**
            `_log_memory_usage(context_str) :=`
            `  If PSUTIL_AVAILABLE Then`
            `    rss_mb := GetProcessMemoryRSS_MB()`
            `    vms_mb := GetProcessMemoryVMS_MB()`
            `    Log(level='debug', message='{context_str} Memory Usage: RSS={rss_mb}MB, VMS={vms_mb}MB')`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, logs memory if `psutil` is present.
            - Failures/Incorrect logic: None apparent. The conditional execution is correct.
            - Fix/Clarify: Useful utility. Static method is appropriate as it doesn't depend on instance state (`self`).

    III-22.) Method: `find_duplicate_files(self) -> Dict[str, List[str]]`
        - **III-22-a.) Purpose:**
            - Identifies duplicate files within the repository based on their stored SHA256 hashes.
        - **III-22-b.) Actions:**
            1. Initializes `hashes_seen = collections.defaultdict(list)`. This will map a hash to a list of relative file paths having that hash.
            2. Iterates through all entries (`rel_path`, `data`) in `self.index.items()`:
                - Retrieves the SHA256 hash from `data['hashes']['sha256']`.
                - Appends the `rel_path` to the list associated with this hash in `hashes_seen`.
            3. Initializes `duplicates = {}`.
            4. Iterates through `hashes_seen.items()`:
                - If a hash has more than one file path associated with it (i.e., `len(paths) > 1`), it's a duplicate.
                - Stores this in the `duplicates` dictionary: `{hash: list_of_duplicate_paths}`.
            5. Logs the number of duplicate sets found.
            6. Returns the `duplicates` dictionary.
        - **III-22-c.) Calls to other methods/functions:**
            - `collections.defaultdict()`
            - `log_statement()`
        - **III-22-d.) Mathematical/Logical Formula:**
            `find_duplicate_files(instance) :=`
            `  hash_map := defaultdict(list)`
            `  For (rel_path, data) in instance.index.items():`
            `    sha256_hash := data['hashes']['sha256']`
            `    AddToList(hash_map[sha256_hash], rel_path)`
            `  duplicate_sets := {}`
            `  For (hash_val, paths_list) in hash_map.items():`
            `    If len(paths_list) > 1 Then`
            `      duplicate_sets[hash_val] := paths_list`
            `  Log(level='info', message='Found {len(duplicate_sets)} sets of duplicate files.')`
            `  Return duplicate_sets`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent. Efficiently uses `defaultdict` for grouping.
            - Fix/Clarify: Correct.

    III-23.) Method: `resolve_file_path(self, file_identifier: Union[str, Path]) -> Optional[Path]`
        - **III-23-a.) Purpose and Parameters:**
            - Purpose: Resolves a file identifier, which could be an absolute path or a path relative to the repository root, to an absolute `Path` object within the repository. Primarily useful for converting stored relative paths to actual file system paths.
            - Parameters:
                - `self`: Instance of the class.
                - `file_identifier` (Union[str, Path]): The file path or identifier to resolve.
        - **III-23-b.) Actions:**
            1. Converts `file_identifier` to a `Path` object.
            2. Checks if the `file_identifier` (as a Path) is already absolute.
                - If yes, and it exists, it's returned directly. (This assumes that an absolute path provided should still be checked against the repo, or that any absolute path is fine).
                - If yes, but does not exist, it might imply an issue or that it's a new file not yet on disk.
            3. If not absolute, it's assumed to be a path relative to `self.repository_path`.
                - Constructs the absolute path using `self.repository_path / file_identifier_path`.
            4. The code returns `self._get_absolute_path(str(file_identifier_path))`. This means even if an absolute path was given, it's effectively treated as if it were a relative path to be joined with `self.repository_path` by `_get_absolute_path` if `file_identifier_path` was not already absolute. This part seems a bit convoluted.
            The actual implementation is simpler:
            1. Converts input `file_identifier` to string.
            2. Calls `self._get_absolute_path()` with this string.
            3. If `_get_absolute_path` succeeds, returns its result.
            4. Catches generic `Exception`, logs, and returns `None`.
        - **III-23-c.) Calls to other methods/functions:**
            - `str()`
            - `self._get_absolute_path()`
            - `log_statement()`
        - **III-23-d.) Mathematical/Logical Formula (based on actual code):**
            `resolve_file_path(instance, identifier) :=`
            `  Try:`
            `    path_str := AsString(identifier)`
            `    abs_path := instance._get_absolute_path(path_str)`
            `    Return abs_path`
            `  Catch Exception e:`
            `    Log(level='error', message='Error resolving file path {identifier}: {e}')`
            `    Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, it essentially converts a given identifier (assumed to be a relative path key from the index or a path that *can* be interpreted as relative to the repo root) into a full, absolute path.
            - Failures/Incorrect logic: The docstring implies more complex logic (checking `is_absolute`) than the implementation, which directly passes the stringified identifier to `_get_absolute_path`. `_get_absolute_path` itself simply joins with `self.repository_path`. So, if an absolute path is passed to `resolve_file_path`, the behavior of `self.repository_path / absolute_path_obj` can be unexpected (e.g., `/repo/path` / `/abs/other/path` results in `/abs/other/path`). This might be okay if `file_identifier` is always expected to be a key from the index (which is relative).
            - Fix/Clarify:
                - If `file_identifier` can truly be an already absolute path that should be used as-is (if valid), the logic should be:
                  ```python
                  # p = Path(file_identifier)
                  # if p.is_absolute():
                  #     if p.exists(): return p # Or check if it's within repo bounds
                  #     else: # handle non-existing absolute path
                  # else:
                  #     return self._get_absolute_path(str(file_identifier))
                  ```
                - Given the current implementation, the docstring should be simplified to reflect that it primarily converts known relative identifiers to absolute paths. The name `resolve_file_path` is a bit general for just `_get_absolute_path`.

    III-24.) Method: `_get_processed_filename(self, source_filepath: Path, new_extension: Optional[str] = None) -> str`
        - **III-24-a.) Purpose and Parameters:**
            - Purpose: Generates a filename for a processed version of a source file. Typically appends "_processed" before the extension or replaces the extension.
            - Parameters:
                - `self`: Instance of the class.
                - `source_filepath` (Path): The `Path` object of the original source file.
                - `new_extension` (Optional[str], optional): If provided, replaces the original file's extension. Must include the leading dot (e.g., ".txt").
        - **III-24-b.) Actions:**
            1. Takes the `source_filepath.stem` (filename without final extension).
            2. Appends "_processed" to the stem: `base_name = f"{source_filepath.stem}_processed"`.
            3. If `new_extension` is provided and is a string:
                - Constructs the new filename as `f"{base_name}{new_extension}"`.
            4. Else (no `new_extension` or invalid type):
                - Constructs the new filename by appending the original `source_filepath.suffix` (original extension): `f"{base_name}{source_filepath.suffix}"`.
            5. Logs the generated name and returns it.
        - **III-24-c.) Calls to other methods/functions:**
            - `Path.stem`, `Path.suffix` (attributes of Path object)
            - `log_statement()`
        - **III-24-d.) Mathematical/Logical Formula:**
            `_get_processed_filename(instance, src_path_obj, opt_new_ext) :=`
            `  base := src_path_obj.stem + "_processed"`
            `  If IsString(opt_new_ext) Then`
            `    processed_name := base + opt_new_ext`
            `  Else`
            `    processed_name := base + src_path_obj.suffix`
            `  Log(level='debug', message='Generated processed filename: {processed_name}')`
            `  Return processed_name`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes.
            - Failures/Incorrect logic: None apparent for its defined behavior. The requirement for `new_extension` to include the dot is a convention to be followed by the caller.
            - Fix/Clarify: Clear.

    III-25.) Method: `_determine_processed_path(self, source_filepath_str: str, app_state: Dict[str, Any], new_extension: Optional[str] = None) -> Optional[Path]`
        - **III-25-a.) Purpose and Parameters:**
            - Purpose: Determines the full absolute path for a processed file. It uses a base output directory specified in the application's configuration (via `app_state`) and a generated processed filename.
            - Parameters:
                - `self`: Instance of the class.
                - `source_filepath_str` (str): String representation of the source file path.
                - `app_state` (Dict[str, Any]): The application state dictionary, expected to contain configuration under `app_state['config']['DataProcessingConfig']['output_directory']`.
                - `new_extension` (Optional[str], optional): New extension for the processed file (passed to `_get_processed_filename`).
        - **III-25-b.) Actions:**
            1. Validates that `source_filepath_str` is a non-empty string and `app_state` is a dictionary. If not, logs error and returns `None`.
            2. Converts `source_filepath_str` to a `Path` object.
            3. Calls `self._get_processed_filename()` to get the new filename.
            4. Checks if `app_state` contains 'config' and if it's not empty. Logs error and returns `None` if not.
            5. Retrieves `DataProcessingConfig` from `app_state['config']`.
            6. Retrieves `output_directory` string from `dp_config`. Logs error and returns `None` if not found.
            7. Converts `output_dir_str` to a `Path` object.
            8. Constructs the full processed path by joining `output_dir` with `processed_filename`.
            9. Logs the determined path and returns its resolved (absolute, canonical) version.
            10. Catches `KeyError`, `TypeError`, `ValueError`, or general `Exception` during path construction, logs it (with `exc_info=True`), and returns `None`.
        - **III-25-c.) Calls to other methods/functions:**
            - `Path()`
            - `self._get_processed_filename()`
            - `Path.resolve()`
            - `log_statement()`
        - **III-25-d.) Mathematical/Logical Formula:**
            `_determine_processed_path(instance, src_fp_str, app_state_dict, opt_new_ext) :=`
            `  ValidateInputs(src_fp_str, app_state_dict)`
            `  If Invalid Then Return None`
            `  src_path_obj := Path(src_fp_str)`
            `  proc_filename := instance._get_processed_filename(src_path_obj, opt_new_ext)`
            `  Try:`
            `    output_dir_config_str := app_state_dict['config']['DataProcessingConfig']['output_directory']`
            `    If Not output_dir_config_str Then Return None`
            `    output_dir_path := Path(output_dir_config_str)`
            `    full_proc_path := ResolvePath(output_dir_path / proc_filename)`
            `    Log(level='debug', message='Determined processed path: {full_proc_path}')`
            `    Return full_proc_path`
            `  Catch (KeyError | TypeError | ValueError | Exception e):`
            `    Log(level='error', message='Error constructing processed path: {e}', exc_info=True)`
            `    Return None`
        - **Assessment (Task 1.4):**
            - Accomplishes intended actions: Yes, constructs a path based on external configuration.
            - Failures/Incorrect logic: Relies heavily on the structure of `app_state` and the configuration within it. If the config path changes, this method breaks. This tight coupling is common but something to be aware of.
            - Fix/Clarify: The error handling is good. The dependency on `app_state` structure is explicit.